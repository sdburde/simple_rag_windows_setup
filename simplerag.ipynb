{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61fce3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The Power of Persistence\\n\\nMy friends, today I want to talk to you about one of the most important qualities for success: persistence.\\n\\nLife will knock you down - this is inevitable. But what separates those who achieve their dreams from those who don\\'t is the willingness to get back up. Every failure is a lesson, every setback is setup for a comeback.\\n\\nConsider Thomas Edison\\'s story. He failed thousands of times before inventing the light bulb. When asked about these failures, he famously replied: \"I have not failed. I\\'ve just found 10,000 ways that won\\'t work.\" That is the mindset of persistence.\\n\\nIn your own journey:\\n1. When you face obstacles, see them as opportunities to grow\\n2. When you feel discouraged, remember your why\\n3. When progress seems slow, trust the process\\n\\nPersistence isn\\'t about never falling - it\\'s about always rising. It\\'s not about speed, but direction. Keep moving forward, one step at a time.\\n\\nThe road to success is always under construction, but with persistence as your foundation, you will build something remarkable.\\n\\nThank you.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Injection\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader= TextLoader(\"speech.txt\")\n",
    "text_documents= loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d9047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "#load, chunk, and index the content of the html page\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                      bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                          class_=(\"post-title\",\"post_content\",\"post_header\")\n",
    "\n",
    "                      )))\n",
    "\n",
    "text_documents= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "544d986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pdf reader \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf')\n",
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a628c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents= text_splitter.split_documents(docs)\n",
    "documents[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f0710f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize Ollama Embeddings (make sure Ollama is running locally)\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",  # or another embedding model like \"nomic-embed-text\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize ChatGroq LLM (for chat/completion)\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",  # Updated model name\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb98112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create vector store with the embeddings\n",
    "# db = Chroma.from_documents(\n",
    "#     documents=documents[:15],  # Your loaded documents\n",
    "#     embedding=embeddings,     # Use the embedding model here\n",
    "#     persist_directory=\"./chroma_db\"  # Optional: persist the database\n",
    "# )\n",
    "\n",
    "\n",
    "# # Process documents in smaller batches\n",
    "# batch_size = 5  # Reduce if still crashing\n",
    "# for i in range(0, len(documents), batch_size):\n",
    "#     _ = Chroma.from_documents(\n",
    "#         documents=documents[i:i+batch_size],\n",
    "#         embedding=embeddings,\n",
    "#         persist_directory=\"./chroma_db\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e08240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS  # More memory-efficient\n",
    "\n",
    "db = FAISS.from_documents(\n",
    "    documents=documents[:15],\n",
    "    embedding=embeddings\n",
    ")\n",
    "db.save_local(\"./faiss_db\")  # Save instead of persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5470edc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, it appears that the main theme of the speech is \"Attention\" in the context of natural language processing and deep learning. The documents seem to be a research paper or article discussing various aspects of attention mechanisms in neural networks, including self-attention, multi-head attention, and their applications in tasks such as reading comprehension, abstractive summarization, and language modeling.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "query = \"What is the main theme of the speech?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Use the LLM to generate a response\n",
    "response = llm.invoke(f\"Based on these documents: {docs}, answer: {query}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "431249f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3f84ce69-4192-4036-975a-ac666b4fb37e', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(id='b7c6179a-161e-4aed-93b0-557f9a606571', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       " Document(id='30ce89c6-7cd5-401c-96d2-e70e3176cb8b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'),\n",
       " Document(id='96e2aeb5-1ecc-4a95-8d86-be4f78ccf655', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector database\n",
    "query= \"Who is the authors of the attention is all you need reseach paper\"\n",
    "result= db.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09875df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Vector database\n",
    "query= \"What is attention is all you need\"\n",
    "result= db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efdad30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FAISS Vector Database\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# db1= FAISS.from_documents(documents[:20], embedding=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
